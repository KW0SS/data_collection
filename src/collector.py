"""ë°ì´í„° ìˆ˜ì§‘ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´í„°.

ê¸°ì—… ëª©ë¡(CSV or ì§ì ‘ ì…ë ¥) + ì—°ë„ ë²”ìœ„ â†’ ë¶„ê¸°ë³„ ì¬ë¬´ë¹„ìœ¨ CSV ì¶œë ¥.
ì„ íƒì ìœ¼ë¡œ ì›ë³¸ ì¬ë¬´ì œí‘œ JSONì„ ë¡œì»¬(data/raw/) ë˜ëŠ” S3ì— ì €ì¥.
"""

from __future__ import annotations

import csv
import json
import sys
import time
from pathlib import Path
from typing import Any

from .dart_api import (
    DartApiError,
    REPORT_CODES,
    fetch_financial_statements,
    get_api_key,
    resolve_corp_code,
)
from .account_mapper import extract_standard_items
from .ratio_calculator import RATIO_NAMES, compute_all_ratios
from .s3_uploader import upload_batch_to_s3

# â”€â”€ ê¸°ë³¸ ê²½ë¡œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_DIR = Path(__file__).resolve().parent.parent / "data"
OUTPUT_DIR = DATA_DIR / "output"
INPUT_DIR = DATA_DIR / "input"
RAW_DIR = DATA_DIR / "raw"


def _ensure_dirs(save_raw: bool = False) -> None:
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    INPUT_DIR.mkdir(parents=True, exist_ok=True)
    if save_raw:
        RAW_DIR.mkdir(parents=True, exist_ok=True)


def _save_raw_json(
    raw_items: list[dict[str, Any]],
    stock_code: str,
    year: str,
    quarter: str,
    fs_div: str,
) -> None:
    """ì›ë³¸ ì¬ë¬´ì œí‘œ JSONì„ data/raw/ í´ë”ì— ì €ì¥."""
    filename = f"{stock_code}_{year}_{quarter}_{fs_div}.json"
    path = RAW_DIR / filename
    path.write_text(
        json.dumps(raw_items, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )


# â”€â”€ CSV ì…ë ¥ íŒŒì‹± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_company_list(csv_path: Path) -> list[dict[str, str]]:
    """
    ê¸°ì—… ëª©ë¡ CSV ì½ê¸°.

    CSV ì»¬ëŸ¼ (í—¤ë” í•„ìˆ˜):
      stock_code   â€“ ì¢…ëª©ì½”ë“œ (6ìë¦¬, ì˜ˆ: 005930)
      corp_name    â€“ ê¸°ì—…ëª… (ì°¸ê³ ìš©)
      label        â€“ 0=ì •ìƒ, 1=ìƒí (ì°¸ê³ ìš©, ìˆ˜ì§‘ì—ëŠ” ë¯¸ì‚¬ìš©)

    Optional:
      corp_code    â€“ DART ê³ ìœ ì½”ë“œ (ìˆìœ¼ë©´ API ë³€í™˜ ìƒëµ)
    """
    if not csv_path.exists():
        raise FileNotFoundError(f"ê¸°ì—… ëª©ë¡ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {csv_path}")

    rows: list[dict[str, str]] = []
    with open(csv_path, newline="", encoding="utf-8-sig") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append({k.strip(): (v or "").strip() for k, v in row.items()})
    return rows


# â”€â”€ ë‹¨ì¼ ê¸°ì—…Â·ë‹¨ì¼ ë³´ê³ ì„œ ì²˜ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_single(
    api_key: str,
    corp_code: str,
    stock_code: str,
    corp_name: str,
    year: str,
    quarter: str,
    fs_div: str = "CFS",
    save_raw: bool = False,
) -> tuple[dict[str, Any], list[dict[str, Any]]]:
    """
    í•œ ê¸°ì—…ì˜ í•œ ë¶„ê¸° ì¬ë¬´ë¹„ìœ¨ì„ ìˆ˜ì§‘.

    Returns:
        (ratio_row, raw_items) íŠœí”Œ.
        ratio_row: {"stock_code": ..., "corp_name": ..., "ë¹„ìœ¨1": val, ...}
        raw_items: DART ì›ë³¸ ë°ì´í„° (ë°ì´í„° ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
    """
    reprt_code = REPORT_CODES[quarter]
    try:
        raw_items = fetch_financial_statements(
            api_key, corp_code, year, reprt_code, fs_div
        )
    except DartApiError as e:
        print(f"  âš  API ì˜¤ë¥˜ ({corp_name} {year}-{quarter}): {e}", file=sys.stderr)
        raw_items = []

    if raw_items and save_raw:
        _save_raw_json(raw_items, stock_code, year, quarter, fs_div)

    if not raw_items:
        ratios = {name: None for name in RATIO_NAMES}
    else:
        std_items = extract_standard_items(raw_items)
        ratios = compute_all_ratios(std_items)

    row: dict[str, Any] = {
        "stock_code": stock_code,
        "corp_name": corp_name,
        "year": year,
        "quarter": quarter,
    }
    row.update(ratios)
    return row, raw_items


# â”€â”€ ë°°ì¹˜ ìˆ˜ì§‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def collect_batch(
    stock_codes: list[str] | None = None,
    corp_codes: list[str] | None = None,
    years: list[str] | None = None,
    quarters: list[str] | None = None,
    companies_csv: Path | None = None,
    fs_div: str = "CFS",
    output_dir: Path | None = None,
    api_key: str | None = None,
    delay: float = 0.5,
    save_raw: bool = False,
    upload_s3: bool = False,
    s3_bucket: str | None = None,
    s3_region: str | None = None,
) -> list[Path]:
    """
    ì—¬ëŸ¬ ê¸°ì—… Ã— ì—°ë„ Ã— ë¶„ê¸°ì˜ ì¬ë¬´ë¹„ìœ¨ì„ ìˆ˜ì§‘í•˜ì—¬ CSV ì €ì¥.

    íŒŒì¼ëª… ê·œì¹™: {ì¢…ëª©ì½”ë“œ}_{ì—°ë„}.csv  (ì˜ˆ: 019440_2023.csv)
    ê° ê¸°ì—… Ã— ì—°ë„ë³„ë¡œ ë³„ë„ì˜ CSV íŒŒì¼ë¡œ ì €ì¥ë©ë‹ˆë‹¤.

    ì‚¬ìš© ë°©ì‹ ë‘ ê°€ì§€:
    1) companies_csv ì§€ì • â†’ CSVì—ì„œ ê¸°ì—… ëª©ë¡ ë¡œë“œ
    2) stock_codes/corp_codes ì§ì ‘ ì „ë‹¬

    Args:
        stock_codes: ì¢…ëª©ì½”ë“œ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["005930", "035720"])
        corp_codes: DART ê³ ìœ ì½”ë“œ ë¦¬ìŠ¤íŠ¸
        years: ìˆ˜ì§‘ ì—°ë„ ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["2022", "2023"])
        quarters: ë¶„ê¸° ë¦¬ìŠ¤íŠ¸ (ì˜ˆ: ["Q1", "H1", "Q3", "ANNUAL"])
        companies_csv: ê¸°ì—… ëª©ë¡ CSV íŒŒì¼ ê²½ë¡œ
        fs_div: "CFS" (ì—°ê²°) ë˜ëŠ” "OFS" (ë³„ë„)
        output_dir: ê²°ê³¼ CSV ì €ì¥ ë””ë ‰í„°ë¦¬ (ê¸°ë³¸: data/output/)
        api_key: DART API í‚¤
        delay: API í˜¸ì¶œ ê°„ ëŒ€ê¸° ì‹œê°„(ì´ˆ)
        save_raw: ì›ë³¸ ì¬ë¬´ì œí‘œ JSONì„ data/raw/ì— ì €ì¥í• ì§€ ì—¬ë¶€
        upload_s3: ì›ë³¸ ì¬ë¬´ì œí‘œë¥¼ S3ì— ì—…ë¡œë“œí• ì§€ ì—¬ë¶€
        s3_bucket: S3 ë²„í‚· ì´ë¦„ (ì—†ìœ¼ë©´ .envì—ì„œ ì½ê¸°)
        s3_region: AWS ë¦¬ì „ (ì—†ìœ¼ë©´ .envì—ì„œ ì½ê¸°)

    Returns:
        ì €ì¥ëœ CSV íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    _ensure_dirs(save_raw=save_raw)
    key = get_api_key(api_key)

    if quarters is None:
        quarters = list(REPORT_CODES.keys())
    if years is None:
        years = ["2023"]

    # ê¸°ì—… ëª©ë¡ êµ¬ì„±
    companies: list[dict[str, str]] = []

    if companies_csv:
        companies = load_company_list(companies_csv)
    elif stock_codes:
        for sc in stock_codes:
            companies.append({"stock_code": sc, "corp_name": "", "corp_code": ""})
    elif corp_codes:
        for cc in corp_codes:
            companies.append({"stock_code": "", "corp_name": "", "corp_code": cc})
    else:
        raise ValueError("ê¸°ì—… ëª©ë¡ì´ ì—†ìŠµë‹ˆë‹¤. stock_codes, corp_codes, ë˜ëŠ” companies_csvë¥¼ ì§€ì •í•˜ì„¸ìš”.")

    # corp_code í™•ë³´
    for comp in companies:
        if not comp.get("corp_code"):
            try:
                comp["corp_code"] = resolve_corp_code(
                    key,
                    stock_code=comp.get("stock_code"),
                    corp_name=comp.get("corp_name") or None,
                )
            except DartApiError as e:
                print(f"  âš  ê¸°ì—…ì½”ë“œ í™•ì¸ ì‹¤íŒ¨ ({comp}): {e}", file=sys.stderr)
                comp["corp_code"] = ""

    # ê²°ê³¼ ìˆ˜ì§‘
    all_rows: list[dict[str, Any]] = []
    s3_upload_queue: list[dict[str, Any]] = []  # S3 ì—…ë¡œë“œ ëŒ€ê¸°ì—´
    total = len(companies) * len(years) * len(quarters)
    done = 0

    for comp in companies:
        cc = comp.get("corp_code", "")
        sc = comp.get("stock_code", "")
        cn = comp.get("corp_name", "")
        gics = comp.get("gics_sector", "Unknown")
        if not cc:
            print(f"  â­ ê±´ë„ˆëœ€ (corp_code ì—†ìŒ): {sc} {cn}", file=sys.stderr)
            done += len(years) * len(quarters)
            continue

        for yr in years:
            for q in quarters:
                done += 1
                print(f"  [{done}/{total}] {cn or sc} {yr}-{q} ...", file=sys.stderr)

                # CFS ì‹œë„ â†’ ì‹¤íŒ¨í•˜ë©´ OFS í´ë°±
                row, raw_items = collect_single(key, cc, sc, cn, yr, q, fs_div, save_raw=save_raw)
                # CFSì—ì„œ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìœ¼ë©´ OFSë¡œ ì¬ì‹œë„
                if fs_div == "CFS" and all(row.get(n) is None for n in RATIO_NAMES):
                    row, raw_items = collect_single(key, cc, sc, cn, yr, q, "OFS", save_raw=save_raw)

                row["label"] = comp.get("label", "")
                all_rows.append(row)

                # S3 ì—…ë¡œë“œ ëŒ€ê¸°ì—´ì— ì¶”ê°€
                if upload_s3 and raw_items:
                    s3_upload_queue.append({
                        "raw_items": raw_items,
                        "stock_code": sc,
                        "year": yr,
                        "quarter": q,
                        "gics_sector": gics,
                    })

                if delay > 0:
                    time.sleep(delay)

    # â”€â”€ CSV ì €ì¥: ê¸°ì—…ì½”ë“œ_ì—°ë„.csv ë³„ë¡œ ë¶„ë¦¬ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    save_dir = output_dir if output_dir else OUTPUT_DIR
    save_dir.mkdir(parents=True, exist_ok=True)

    # (stock_code, year) ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í•‘
    from collections import defaultdict
    groups: dict[tuple[str, str], list[dict[str, Any]]] = defaultdict(list)
    for row in all_rows:
        key = (row["stock_code"], row["year"])
        groups[key].append(row)

    fieldnames = ["stock_code", "corp_name", "year", "quarter", "label"] + RATIO_NAMES
    saved_files: list[Path] = []

    for (sc, yr), rows in groups.items():
        filename = f"{sc}_{yr}.csv"
        filepath = save_dir / filename
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, extrasaction="ignore")
            writer.writeheader()
            for row in rows:
                writer.writerow(row)
        saved_files.append(filepath)
        print(f"  ğŸ“„ {filepath}  ({len(rows)}í–‰)", file=sys.stderr)

    print(f"\nâœ… CSV ì €ì¥ ì™„ë£Œ: {save_dir}/  (ì´ {len(saved_files)}ê°œ íŒŒì¼, {len(all_rows)}í–‰)", file=sys.stderr)

    # â”€â”€ S3 ì—…ë¡œë“œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if upload_s3 and s3_upload_queue:
        print(f"\nâ˜ï¸  S3 ì—…ë¡œë“œ ì‹œì‘ ({len(s3_upload_queue)}ê°œ íŒŒì¼)...", file=sys.stderr)
        upload_batch_to_s3(s3_upload_queue, bucket=s3_bucket, region=s3_region)

    return saved_files
